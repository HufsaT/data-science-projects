{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying RNN to Ecommerce Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a stable LSTM on dataset (ignore, WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"stateful_binary_lstm_test\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_90 (LSTM)               (1, 11, 100)              40800     \n",
      "_________________________________________________________________\n",
      "lstm_91 (LSTM)               (1, 100)                  80400     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (1, 100)                  0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (1, 1)                    101       \n",
      "=================================================================\n",
      "Total params: 121,301\n",
      "Trainable params: 121,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# stateful LSTM\n",
    "def stateful_LSTM(inputA, initial_bias_, batch_size, lahead):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.LSTM(100, input_shape=[lahead, inputA], stateful=True, batch_size=batch_size, return_sequences=True),\n",
    "        keras.layers.LSTM(100),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation='sigmoid', bias_initializer=initial_bias_)\n",
    "])    \n",
    "    return model\n",
    "\n",
    "tsteps = 2\n",
    "\n",
    "# The input sequence length that the LSTM is trained on for each output point\n",
    "lahead = 11\n",
    "\n",
    "# training parameters passed to \"model.fit(...)\"\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "\n",
    "binary_model = stateful_LSTM(1, initial_bias, batch_size, lahead)\n",
    "binary_model._name='stateful_binary_lstm_test'\n",
    "binary_model.summary()\n",
    "binary_model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "                    metrics=['binary_crossentropy', 'Precision','Recall', 'accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 37s 12ms/sample - loss: 0.6125 - binary_crossentropy: 0.6125 - Precision: 0.3611 - Recall: 0.0148 - accuracy: 0.7030 - val_loss: 0.6158 - val_binary_crossentropy: 0.6158 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 2 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 33s 11ms/sample - loss: 0.6108 - binary_crossentropy: 0.6108 - Precision: 0.3750 - Recall: 0.0068 - accuracy: 0.7050 - val_loss: 0.6147 - val_binary_crossentropy: 0.6147 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 3 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 34s 11ms/sample - loss: 0.6080 - binary_crossentropy: 0.6080 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6158 - val_binary_crossentropy: 0.6158 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 4 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 33s 11ms/sample - loss: 0.6069 - binary_crossentropy: 0.6069 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6152 - val_binary_crossentropy: 0.6152 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 5 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 32s 11ms/sample - loss: 0.6065 - binary_crossentropy: 0.6065 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6150 - val_binary_crossentropy: 0.6150 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 6 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 31s 10ms/sample - loss: 0.6059 - binary_crossentropy: 0.6059 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6145 - val_binary_crossentropy: 0.6145 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 7 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 32s 11ms/sample - loss: 0.6057 - binary_crossentropy: 0.6057 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6146 - val_binary_crossentropy: 0.6146 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 8 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 32s 11ms/sample - loss: 0.6057 - binary_crossentropy: 0.6057 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6146 - val_binary_crossentropy: 0.6146 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 9 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 32s 11ms/sample - loss: 0.6057 - binary_crossentropy: 0.6057 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6146 - val_binary_crossentropy: 0.6146 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n",
      "Epoch 10 / 10\n",
      "Train on 3000 samples, validate on 500 samples\n",
      "3000/3000 [==============================] - 32s 11ms/sample - loss: 0.6056 - binary_crossentropy: 0.6056 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.7063 - val_loss: 0.6146 - val_binary_crossentropy: 0.6146 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 0.6960\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    print('Epoch', i + 1, '/', epochs)\n",
    "    # Note that the last state for sample i in a batch will\n",
    "    # be used as initial state for sample i in the next batch.\n",
    "    # Thus we are simultaneously training on batch_size series with\n",
    "    # lower resolution than the original series contained in data_input.\n",
    "    # Each of these series are offset by one step and can be\n",
    "    # extracted with data_input[i::batch_size].\n",
    "    binary_model.fit(x3_train, y3_train, \n",
    "                       validation_data=(x3_val, y3_val),\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=1,\n",
    "                       verbose=1,\n",
    "                       shuffle=False)\n",
    "    binary_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: (500, 1) y_actual: (500, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82       348\n",
      "           1       0.00      0.00      0.00       152\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.35      0.50      0.41       500\n",
      "weighted avg       0.48      0.70      0.57       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/htahir/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500, 1)"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = binary_model.predict_classes(x3_val[:,:,:], batch_size=batch_size)\n",
    "y_actual = y3_val[:,:]\n",
    "\n",
    "print('y_pred: {} y_actual: {}'.format(y_pred.shape, y_actual.shape))\n",
    "print(classification_report(y_actual.flatten(), y_pred.flatten()))\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for Ecommerce Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to use an RNN to predict customer conversion, using the behavioural features of the customers visiting the website over several days. At the end, the model should be able to tell us if the user, based on their actions, is likely to convert on a specific timestep (binary classification).\n",
    "\n",
    "Unlike the traditional machine-learning algorithm we also use in this notebook, the input for this NN model will be a bit different. It will need to be three-dimensional and sequential, per user, in an array of shape:  \n",
    "\n",
    "**[visitors, timesteps, features per timestep]**\n",
    "\n",
    "Our output will be a vector of shape [visitors, target_class]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports here\n",
    "\n",
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "%load_ext tensorboard\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
    "if IS_COLAB:\n",
    "    print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some basic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll define some useful functiosn that we'll be reusing later. \n",
    "\n",
    "Function below simply creates a palce for us to drop the logs for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(folder_name):\n",
    "    root_logdir = os.path.join(os.curdir, folder_name)\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "def prep_run():\n",
    "    keras.backend.clear_session()\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    \n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.axis([1, 20, 0, 0.05])\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform our dataset for keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 9 columns):\n",
      "timestamp        500000 non-null object\n",
      "visitorid        500000 non-null int64\n",
      "event            500000 non-null object\n",
      "itemid           500000 non-null int64\n",
      "transactionid    4185 non-null float64\n",
      "categoryid       455357 non-null float64\n",
      "parentid         455354 non-null float64\n",
      "sessionid        500000 non-null object\n",
      "date             500000 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(3), int64(2), object(3)\n",
      "memory usage: 34.3+ MB\n"
     ]
    }
   ],
   "source": [
    "pwd = '/Users/htahir/Documents/SCS_machine_learning/Final_Project/'\n",
    "\n",
    "# load our clean dataset:\n",
    "\n",
    "df = pd.read_csv(pwd+'ecommerce_merged_data.csv', nrows=500000,infer_datetime_format=True, parse_dates=[8])\n",
    "# df = df.query('visitorid in (552148, 189384)').copy()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the Keras model is to predict conversion vs non-conversion of a user based on their historical buying behaviour. We will not train the model on exactly WHAT item the user is buying (because there are literally over a 100,000 of them). However, we'll train on on the parent categories of an item a visitor has bought from historically.\n",
    "\n",
    "Let's load our clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "      <th>categoryid</th>\n",
       "      <th>parentid</th>\n",
       "      <th>sessionid</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-02 01:02:12</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>2015-06-02_257597</td>\n",
       "      <td>2015-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-06-02 01:50:14</td>\n",
       "      <td>992329</td>\n",
       "      <td>view</td>\n",
       "      <td>248676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1231.0</td>\n",
       "      <td>901.0</td>\n",
       "      <td>2015-06-02_992329</td>\n",
       "      <td>2015-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-02 01:13:19</td>\n",
       "      <td>111016</td>\n",
       "      <td>view</td>\n",
       "      <td>318965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-06-02_111016</td>\n",
       "      <td>2015-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-02 01:12:35</td>\n",
       "      <td>483717</td>\n",
       "      <td>view</td>\n",
       "      <td>253185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>914.0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>2015-06-02_483717</td>\n",
       "      <td>2015-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-02 01:02:17</td>\n",
       "      <td>951259</td>\n",
       "      <td>view</td>\n",
       "      <td>367447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>491.0</td>\n",
       "      <td>679.0</td>\n",
       "      <td>2015-06-02_951259</td>\n",
       "      <td>2015-06-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  visitorid event  itemid  transactionid  categoryid  \\\n",
       "0  2015-06-02 01:02:12     257597  view  355908            NaN      1173.0   \n",
       "1  2015-06-02 01:50:14     992329  view  248676            NaN      1231.0   \n",
       "2  2015-06-02 01:13:19     111016  view  318965            NaN         NaN   \n",
       "3  2015-06-02 01:12:35     483717  view  253185            NaN       914.0   \n",
       "4  2015-06-02 01:02:17     951259  view  367447            NaN       491.0   \n",
       "\n",
       "   parentid          sessionid       date  \n",
       "0     805.0  2015-06-02_257597 2015-06-02  \n",
       "1     901.0  2015-06-02_992329 2015-06-02  \n",
       "2       NaN  2015-06-02_111016 2015-06-02  \n",
       "3     226.0  2015-06-02_483717 2015-06-02  \n",
       "4     679.0  2015-06-02_951259 2015-06-02  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to build a transformer to quickly clean up our dataset, dropping any unwaved columns and changing data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformer to drop any useless data\n",
    "\n",
    "class DropColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def transform(self, X, y=None):\n",
    "        df = X.drop(columns=self.cols)\n",
    "        return df\n",
    "    \n",
    "class DropNulls(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, drop_nulls=True):\n",
    "        self.drop_nulls = drop_nulls\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def transform(self, X, y=None):\n",
    "        if self.drop_nulls==True:\n",
    "            df = X.dropna(inplace=False)\n",
    "            df = df.reset_index(inplace=False, drop=True)\n",
    "            return df\n",
    "        else:\n",
    "            pass\n",
    "        return df\n",
    "    \n",
    "class ChangeColType(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols, newType):\n",
    "        self.cols = cols\n",
    "        self.newType = newType\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "    def transform(self, X, y=None):\n",
    "        for col in self.cols:\n",
    "            X[col] = X[col].astype(self.newType)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45682 entries, 0 to 45681\n",
      "Data columns (total 6 columns):\n",
      "visitorid     45682 non-null int64\n",
      "event         45682 non-null object\n",
      "itemid        45682 non-null int64\n",
      "categoryid    45682 non-null int64\n",
      "parentid      45682 non-null int64\n",
      "date          45682 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(1)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "CleanRawData = make_pipeline(\n",
    "    DropColumn(['transactionid', 'sessionid', 'timestamp']),\n",
    "    DropNulls(),\n",
    "    ChangeColType(['categoryid', 'parentid'],int)\n",
    ")\n",
    "df = CleanRawData.transform(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineer our dataset for Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do some quick feature engineering before we start reshaping our dataset for a neural network. This will allow us to provide the model some additional information.  \n",
    "\n",
    "We're going to add weekday as a feature, as ther eis a good chance of a wekly buying patttern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a transformer to extract weekday as a feature\n",
    "\n",
    "class WeekdayExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        weekday = X.progress_apply(lambda x: 'weekday_'+ str(x.weekday()))\n",
    "        return weekday.rename('weekday').to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa48eab3b7e47b5a1a73357c188f770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=500000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 16 columns):\n",
      "timestamp        -500000 non-null object\n",
      "visitorid        -500000 non-null int64\n",
      "event            -500000 non-null object\n",
      "itemid           -500000 non-null int64\n",
      "transactionid    -995815 non-null float64\n",
      "categoryid       -544643 non-null float64\n",
      "parentid         -544646 non-null float64\n",
      "sessionid        -500000 non-null object\n",
      "date             -500000 non-null datetime64[ns]\n",
      "x0_weekday_0     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_1     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_2     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_3     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_4     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_5     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_6     -500000 non-null Sparse[float64, 0.0]\n",
      "dtypes: Sparse[float64, 0.0](7), datetime64[ns](1), float64(3), int64(2), object(3)\n",
      "memory usage: 40.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# pipeline to extract weekday\n",
    "wkday_pipeline = Pipeline(steps=[\n",
    "    ('wkday', WeekdayExtractor()),\n",
    "    ('onehot', OneHotEncoder())\n",
    "])\n",
    "\n",
    "transformed = wkday_pipeline.fit_transform(df['date'])\n",
    "ohe_df = pd.DataFrame.sparse.from_spmatrix(transformed, \n",
    "                                           columns=wkday_pipeline['onehot'].get_feature_names())\n",
    "\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258583,)"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.visitorid.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our keras model needs the dataset to be in a 3D array of shape (visitors, num visits/timesteps, features).\n",
    "\n",
    "The way our dataset is set up, we can't just split the \"events\" data into train and test. We have to preseve the history of each user (i.e. users must not be split across train and test).\n",
    "\n",
    "So here is the plan:\n",
    "\n",
    "1. Create a daily-level dataframe of featuers per visitor:  \n",
    "    a. Pivot entire dataset on daily visitor level so that parentIDs become columns with view counts and add to cart counts per each parentID (we'll make separate DFs for each event type).  \n",
    "    b. To avoid differing numbers of parentID columns in current and any future data, we're going to join it to an empty dataframe that contains column of every single parentID in our dataset).  \n",
    "    c. Combine our views DF and addtoCart DF to create a final daily record per user.\n",
    "2. Create timestep arrays: \n",
    "    a. find max timesteps (this will be the length of our timestep array).  \n",
    "    b. create a list per user containing lists of features per timestep.  \n",
    "    c. Combine the lists to create our final array with a row for every visitor's timestep sequence.\n",
    "    d. Split out into train/test.\n",
    "3. Preprocess our training data.  \n",
    "    a. Combine every feature vector into one input array and pad it to same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a daily-level dataframe of features per visitor:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find some sample users who converted, to avoid running the entire dataset while we work on trasnforming it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our features array, we'll create 3 aggregated DFs on visitor and date level (one for each event: view, add to cart, purchase). One will count # views per categoryid per user daily, the second will count add to cart, the third will count purchases per visitor per day, which we'll convert to 0/1. In our case, we don't care what the user purchased, just that they made a purchase, and that's what we want our model to predict.\n",
    "\n",
    "We'll also create a fourth dataframe that is empty and contains a column for every parent ID in our dataset. This will be merged individually with the views and add to cart dataframes to ensure all our input data is of equal length (and that we never miss a categoryID regardless of if our current dataset has it). So now we ahve two arrays, both of equal number of columns. \n",
    "\n",
    "Now we join them together to create a DF twice as wide (with a column for every category's view and addtocart values separately). This will give us daly-level activity for every single user.\n",
    "\n",
    "The custom transformer below does all this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500000 entries, 0 to 499999\n",
      "Data columns (total 16 columns):\n",
      "timestamp        -500000 non-null object\n",
      "visitorid        -500000 non-null int64\n",
      "event            -500000 non-null object\n",
      "itemid           -500000 non-null int64\n",
      "transactionid    -995815 non-null float64\n",
      "categoryid       -544643 non-null float64\n",
      "parentid         -544646 non-null float64\n",
      "sessionid        -500000 non-null object\n",
      "date             -500000 non-null datetime64[ns]\n",
      "x0_weekday_0     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_1     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_2     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_3     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_4     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_5     -500000 non-null Sparse[float64, 0.0]\n",
      "x0_weekday_6     -500000 non-null Sparse[float64, 0.0]\n",
      "dtypes: Sparse[float64, 0.0](7), datetime64[ns](1), float64(3), int64(2), object(3)\n",
      "memory usage: 40.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeDataset(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # create an empty df with all category vals\n",
    "        self.cat_df = pd.DataFrame(columns=X.parentid.unique().tolist())\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        # create 3 dfs for views, adds, purch\n",
    "        \n",
    "        # views\n",
    "        # removing sample visitorid in (552148, 189384) \n",
    "        print('Creating views pivot table...')\n",
    "        self.df_sample = X.query('event == \"view\" ').copy()\n",
    "        self.df_views = pd.pivot_table(self.df_sample, index=['date','visitorid',\n",
    "                                                              'x0_weekday_0','x0_weekday_1',\n",
    "                                                              'x0_weekday_2','x0_weekday_3',\n",
    "                                                              'x0_weekday_4','x0_weekday_5', \n",
    "                                                              'x0_weekday_6'\n",
    "                                                             ], \n",
    "                                columns=['parentid'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_views = self.df_views.reset_index()\n",
    "        print('views df shape: ',self.df_views.shape)\n",
    "        \n",
    "        # add to cart\n",
    "        print('Creating add-to-cart pivot table...')\n",
    "        self.df_sample2 = X.query('event == \"addtocart\" ').copy()\n",
    "        self.df_adds = pd.pivot_table(self.df_sample2, index=['date','visitorid'], \n",
    "                                columns=['parentid'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_adds = self.df_adds.reset_index()\n",
    "        print('adds df shape: ',self.df_adds.shape)\n",
    "\n",
    "        # purchases\n",
    "        print('Creating transactions pivot table...')\n",
    "        self.df_sample3 = X.query('event == \"transaction\" ').copy()\n",
    "        self.df_convert = pd.pivot_table(self.df_sample3, index=[ 'date','visitorid'], \n",
    "                                columns=['event'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_convert = self.df_convert.reset_index()\n",
    "        \n",
    "        # join views and cart to category df \n",
    "        print('Merging parentID dataframe to views and add-to-cart separately...')\n",
    "        self.df_fullview = self.df_views.merge(self.cat_df, how='outer', on=None)\n",
    "        self.df_fulladds = pd.merge(self.df_adds, self.cat_df, how='outer', on=None)\n",
    "        \n",
    "        # merge both views and cart together\n",
    "        print('...and merging both of them together.')\n",
    "        self.full_df = self.df_fullview.merge(self.df_fulladds, \n",
    "                            how='outer', on=['date','visitorid'], \n",
    "                            suffixes=('_views', '_adds'))\n",
    "        \n",
    "        print('Creating target vector...')\n",
    "        # create our target vector by changing column values to binary\n",
    "        self.df_convert['transaction'] = np.where(self.df_convert['transaction']>0,1,0)\n",
    "        \n",
    "        # use this to find bias value\n",
    "#         print('Calculating bias...')\n",
    "#         self.initial_bias = self.get_bias(self.df_convert['transaction'])\n",
    "        \n",
    "        # merge target DF with the main features DF to make it easier \n",
    "        # to create our final input and target arrays.\n",
    "        self.full_df = self.full_df.merge(self.df_convert, how=\"outer\", on=[\"visitorid\", \"date\"])\n",
    "        self.full_df.fillna(0, inplace=True)\n",
    "        \n",
    "        print('Returning results. Proces complete.')\n",
    "        return self.full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating views pivot table...\n",
      "views df shape:  (252415, 272)\n",
      "Creating add-to-cart pivot table...\n",
      "adds df shape:  (7561, 209)\n",
      "Creating transactions pivot table...\n",
      "Merging parentID dataframe to views and add-to-cart separately...\n",
      "...and merging both of them together.\n",
      "Creating target vector...\n",
      "Returning results. Proces complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(253119, 538)"
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ReshapeDataset().fit_transform(df)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>x0_weekday_0</th>\n",
       "      <th>x0_weekday_1</th>\n",
       "      <th>x0_weekday_2</th>\n",
       "      <th>x0_weekday_3</th>\n",
       "      <th>8_views</th>\n",
       "      <th>9_views</th>\n",
       "      <th>14_views</th>\n",
       "      <th>20_views</th>\n",
       "      <th>...</th>\n",
       "      <th>336_adds</th>\n",
       "      <th>732_adds</th>\n",
       "      <th>1669_adds</th>\n",
       "      <th>1374_adds</th>\n",
       "      <th>275_adds</th>\n",
       "      <th>1632_adds</th>\n",
       "      <th>265_adds</th>\n",
       "      <th>140_adds</th>\n",
       "      <th>1591_adds</th>\n",
       "      <th>transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>4078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>10946</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>27647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-06-01</td>\n",
       "      <td>50966</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 493 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  visitorid  x0_weekday_0  x0_weekday_1  x0_weekday_2  \\\n",
       "0 2015-06-01         17           1.0           0.0           0.0   \n",
       "1 2015-06-01       4078           1.0           0.0           0.0   \n",
       "2 2015-06-01      10946           1.0           0.0           0.0   \n",
       "3 2015-06-01      27647           1.0           0.0           0.0   \n",
       "4 2015-06-01      50966           1.0           0.0           0.0   \n",
       "\n",
       "   x0_weekday_3  8_views  9_views  14_views  20_views  ...  336_adds  \\\n",
       "0           0.0      0.0      0.0       0.0       0.0  ...         0   \n",
       "1           0.0      0.0      0.0       0.0       0.0  ...         0   \n",
       "2           0.0      0.0      0.0       0.0       0.0  ...         0   \n",
       "3           0.0      0.0      0.0       0.0       0.0  ...         0   \n",
       "4           0.0      0.0      0.0       0.0       0.0  ...         0   \n",
       "\n",
       "   732_adds  1669_adds  1374_adds  275_adds  1632_adds  265_adds  140_adds  \\\n",
       "0         0          0          0         0          0         0         0   \n",
       "1         0          0          0         0          0         0         0   \n",
       "2         0          0          0         0          0         0         0   \n",
       "3         0          0          0         0          0         0         0   \n",
       "4         0          0          0         0          0         0         0   \n",
       "\n",
       "   1591_adds  transaction  \n",
       "0          0          0.0  \n",
       "1          0          0.0  \n",
       "2          0          0.0  \n",
       "3          0          0.0  \n",
       "4          0          0.0  \n",
       "\n",
       "[5 rows x 493 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()\n",
    "# dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timestep array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take that full dataset with every parentID, and reshape it into a list of lists of lists:\n",
    "\n",
    "- outer list: contains a row per user  \n",
    "- each row: contains a list per timestep  \n",
    "- timestep list: contains a list of features for that timestep  \n",
    "\n",
    "While creating the features array, we'll also handle the target array, which wil have the same form of being a nested list of lists (except that it will only have one \"feature\" per timestep per user - the class).\n",
    "\n",
    "In the end, we'll have two lists: one for features, one for targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateInputArray(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # find max timesteps\n",
    "        self.max_timesteps = X.groupby('visitorid')['date'].count().max()\n",
    "        print(self.max_timesteps, \" timesteps\")\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        self.users = X.visitorid.unique()\n",
    "        self.user_list = [] \n",
    "        self.target_list = []\n",
    "        \n",
    "        for i in tqdm_notebook(self.users, desc=\"creating features/targets array\"):\n",
    "            self.temp_df = X[X.visitorid == i].copy()\n",
    "            self.t = self.temp_df['transaction'].astype(int).values.tolist()\n",
    "            self.target_list.append(self.t)\n",
    "            self.temp_df.drop(columns=['visitorid', 'date'], inplace=True)\n",
    "            self.record = self.temp_df.values.tolist()\n",
    "            self.user_list.append(self.record)\n",
    "        \n",
    "        print('sequence sample of first visitor:')\n",
    "        print(\"feature array: \", self.user_list[1][0][0:10])\n",
    "        print(\"targets: \",self.target_list[0])\n",
    "        \n",
    "        return self.user_list, self.target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26  timesteps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3762de6d3924897bafedb5f82f3ee3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='creating features/targets array', max=226574.0, style=Pro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sequence sample of first visitor:\n",
      "feature array:  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "targets:  [0]\n"
     ]
    }
   ],
   "source": [
    "features, targets = CreateInputArray().fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done saving.\n"
     ]
    }
   ],
   "source": [
    "# save backup\n",
    "# df_fullview.to_csv('full_views.csv')\n",
    "# df_fulladds.to_csv('full_adds.csv')\n",
    "# print('Done saving.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset at a level where we can split it into train/test withouot splitting a user betwen the two, we'll do that first, and then transform our train set into a time series for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X and Y:  (226574,) (226574,)\n",
      "Splitting out train/test/val.\n",
      "X_train and y_train shapes:\n",
      "(118950,) (118950,)\n",
      "input sample:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "X_val and y_val shapes:\n",
      "(39651,) (39651,)\n",
      "input sample:  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "X_test and y_test shapes:\n",
      "(67973,) (67973,)\n",
      "input sample:  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(features)\n",
    "y = np.array(targets)\n",
    "print(\"X and Y: \" , X.shape, y.shape)\n",
    "\n",
    "# split out data set into train and test\n",
    "print('Splitting out train/test/val.')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "print('X_train and y_train shapes:')\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('input sample: ', X_train[0][0][:10])\n",
    "\n",
    "print('X_val and y_val shapes:')\n",
    "print(X_val.shape, y_val.shape)\n",
    "print('input sample: ', X_val[0][0][:10])\n",
    "\n",
    "print('X_test and y_test shapes:')\n",
    "print(X_test.shape, y_test.shape)\n",
    "print('input sample: ', X_val[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our training data ready to be preprocessed.  \n",
    "Here we decide that for the sake of keeping this a bit more time-efficient, we'll be front-padding all timesteps for users who have fewer than the max num of timesteps. Ideally, we'd properly map the timestep with the day of the period, but that might have to be a latter iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "#### Testing with a fake dataset. Skip to next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0]\n",
      "  [1]]\n",
      "\n",
      " [[0]\n",
      "  [0]]\n",
      "\n",
      " [[0]\n",
      "  [1]]]\n",
      "  Owner  Cat  Age\n",
      "0   Bob    1   10\n",
      "1  Jane    1    3\n",
      "2   Kat    0    2\n",
      "3   Bob    0    3\n",
      "sequence list:\n",
      "[[[1, 10], [0, 3]], [[1, 3]], [[0, 2]]]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame({'Owner': ['Bob', 'Jane', 'Kat', 'Bob'],\n",
    "                        'Cat':[1, 1, 0, 0], \n",
    "                        'Age':[10,3, 2, 3]})\n",
    "test_targets = np.random.randint(2, size=(3, 2, 1))\n",
    "print(test_targets)\n",
    "print(test_df.head())\n",
    "users = test_df.Owner.unique()\n",
    "user_list = [] \n",
    "for i in users:\n",
    "    temp_df = test_df[test_df.Owner == i].copy()\n",
    "#     print(temp_df.head())\n",
    "    temp_df.drop(columns='Owner', inplace=True)\n",
    "    record = temp_df.values.tolist()\n",
    "#     print(record)\n",
    "    user_list.append(record)\n",
    "print('sequence list:')\n",
    "print(user_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1 10]\n",
      "  [ 0  3]]\n",
      "\n",
      " [[ 1  3]\n",
      "  [ 0  0]]\n",
      "\n",
      " [[ 0  2]\n",
      "  [ 0  0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nulayer = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "#     sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre',\n",
    "#     value=0.0\n",
    "# )_df.head()\n",
    "\n",
    "padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(user_list,\n",
    "                                                              padding='post')\n",
    "\n",
    "print(padded_inputs)\n",
    "\n",
    "padded_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ True  True]\n",
      "  [False  True]]\n",
      "\n",
      " [[ True  True]\n",
      "  [False False]]\n",
      "\n",
      " [[False  True]\n",
      "  [False False]]], shape=(3, 2, 2), dtype=bool)\n"
     ]
    }
   ],
   "source": [
    "embedding = keras.layers.Embedding(input_dim=5000, output_dim=16, mask_zero=True)\n",
    "masked_output = embedding(padded_inputs)\n",
    "\n",
    "print(masked_output._keras_mask)\n",
    "# print(masked_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ True  True]\n",
      "  [ True False]\n",
      "  [ True False]]], shape=(1, 3, 2), dtype=bool)\n",
      "tf.Tensor(\n",
      "[[[[ 1. 10.]\n",
      "   [ 0.  3.]]\n",
      "\n",
      "  [[ 1.  3.]\n",
      "   [ 0.  0.]]\n",
      "\n",
      "  [[ 0.  2.]\n",
      "   [ 0.  0.]]]], shape=(1, 3, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "masking_layer = keras.layers.Masking()\n",
    "# Simulate the embedding lookup by expanding the 2D input to 3D,\n",
    "# with embedding dimension of 10.\n",
    "unmasked_embedding = tf.cast(\n",
    "    tf.tile(tf.expand_dims(padded_inputs, axis=0), [1, 1, 1, 1]),\n",
    "    tf.float32)\n",
    "unmasked_embedding\n",
    "masked_embedding = masking_layer(unmasked_embedding)\n",
    "print(masked_embedding._keras_mask)\n",
    "print(masked_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3\n",
      "<tensorflow.python.ops.init_ops_v2.Constant object at 0x1a6fa58b70>\n"
     ]
    }
   ],
   "source": [
    "# test bias\n",
    "classA, classB = np.bincount(test_targets)\n",
    "print(classA, classB)\n",
    "test_bias = np.log(classA/classB)\n",
    "test_bias = tf.keras.initializers.Constant(test_bias)\n",
    "print(test_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_LSTM(inputA, initial_bias_=0.0, **kwargs):\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Masking(input_shape=[None, inputA], mask_value=0),\n",
    "        keras.layers.LSTM(10, return_sequences=True),\n",
    "        keras.layers.LSTM(10, return_sequences=True),\n",
    "#         keras.layers.Dropout(0.3),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(1, activation='sigmoid', bias_initializer=initial_bias_))\n",
    "])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 2)\n",
      "[[[0.99999833]\n",
      "  [0.9824194 ]]\n",
      "\n",
      " [[0.98029053]\n",
      "  [0.5       ]]\n",
      "\n",
      " [[0.9359648 ]\n",
      "  [0.5       ]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = np.array([[[0,0],[0,0],[2,0],[2,1],[1,0],[0,1]],[[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]],\n",
    "                [[2,1],[1,1],[1,2],[2,3],[2,0],[1,10]]])\n",
    "print(a.shape)\n",
    "inputs = tf.keras.Input(shape=(None,2))\n",
    "mask = tf.keras.layers.Masking(mask_value=0)(inputs)\n",
    "out = tf.keras.layers.TimeDistributed(Dense(1,activation='sigmoid'))(mask)\n",
    "model = tf.keras.Model(inputs=inputs,outputs=out)\n",
    "q = model.predict(padded_inputs)\n",
    "print (q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 2, 2), (3, 2, 1))"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_inputs.shape, test_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"test_masking_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_1 (Masking)          (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 10)          520       \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 10)          840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 1)           11        \n",
      "=================================================================\n",
      "Total params: 1,371\n",
      "Trainable params: 1,371\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 9s 5s/sample - loss: 0.4949 - accuracy: 0.6667 - binary_crossentropy: 0.6599 - val_loss: 0.1417 - val_accuracy: 1.0000 - val_binary_crossentropy: 0.2833\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 9ms/sample - loss: 0.4940 - accuracy: 0.6667 - binary_crossentropy: 0.6587 - val_loss: 0.1421 - val_accuracy: 1.0000 - val_binary_crossentropy: 0.2841\n"
     ]
    }
   ],
   "source": [
    "test_masking_model = masking_LSTM(2, test_bias)\n",
    "test_masking_model._name='test_masking_model'\n",
    "print(test_masking_model.summary())\n",
    "\n",
    "test_masking_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", \n",
    "                           metrics=[\"accuracy\", \"binary_crossentropy\"])\n",
    "history = test_masking_model.fit(padded_inputs, test_targets, validation_split=0.25, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"binary_lstm_test\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, None, 2)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 20)          1840      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 20)          3280      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 20)          3280      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 1)           21        \n",
      "=================================================================\n",
      "Total params: 8,421\n",
      "Trainable params: 8,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "./test_lstm_logs/run_2020_04_26-00_41_02\n",
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/2\n",
      "2/2 [==============================] - 14s 7s/sample - loss: 0.4835 - binary_crossentropy: 0.6447 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.2124 - val_binary_crossentropy: 0.4247 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.129816). Check your callbacks.\n",
      "2/2 [==============================] - 0s 79ms/sample - loss: 0.4828 - binary_crossentropy: 0.6437 - Precision: 0.0000e+00 - Recall: 0.0000e+00 - accuracy: 0.6667 - val_loss: 0.2124 - val_binary_crossentropy: 0.4247 - val_Precision: 0.0000e+00 - val_Recall: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a transformer to pad our sequences using Keras's pad_sequences() function. The fit_transform() function will later allow us to ensure our validation and test sets are all padded to the same legnth as our training data.\n",
    "\n",
    "We'll also create a transformer to adjust the dimension of the targets array, as keras needs it to be 3-dimensional to predict a class per timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = {}\n",
    "class PadSequences(BaseEstimator, TransformerMixin):\n",
    "    global timesteps\n",
    "    def __init__(self, padding='pre'):\n",
    "        self.padding = padding\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        df = X\n",
    "#         t = len(df)\n",
    "        t = max([len(df[i]) for i in range(len(df))])\n",
    "        step = {'steps':t}\n",
    "        timesteps.update(step)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df = tf.keras.preprocessing.sequence.pad_sequences(X,padding=self.padding, maxlen=timesteps['steps'])\n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self, X, **kwargs):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "class ExpandDims(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, expandAxis):\n",
    "        self.expandAxis = expandAxis\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = tf.expand_dims(X, axis=self.expandAxis)\n",
    "        return X\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((118950, 26, 536), TensorShape([118950, 26, 1]))"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = PadSequences('pre').fit_transform(X_train)\n",
    "outputPipeline = make_pipeline(\n",
    "    PadSequences('pre'),\n",
    "    ExpandDims(2)\n",
    ")\n",
    "y_train = outputPipeline.transform(y_train)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val shapes:  (39651, 26, 536) (39651, 26, 1)\n",
      "test shapes:  (67973, 26, 536) (67973, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "X_val = PadSequences('pre').transform(X_val)\n",
    "y_val = outputPipeline.transform(y_val)\n",
    "print('val shapes: ',X_val.shape, y_val.shape)\n",
    "\n",
    "X_test = PadSequences('pre').transform(X_test)\n",
    "y_test = outputPipeline.transform(y_test)\n",
    "print('test shapes: ',X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Both input and output arrays are of the same length and dimension per user. We'll use a masking layer as the input layer of our model to ensure the model knows we've padded our inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an LSTM on ecommerce sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to train our model!. But Before we do that, we'll calculate the bias of classes in X_train. This might be used in our model's output layer (where we'll use this bias to tell the moel our data is imbalanced in favour of one class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.800528016188972\n",
      "class 0: 3091434 class 1: 1266\n"
     ]
    }
   ],
   "source": [
    "# recalculating initial bias\n",
    "neg, pos = np.bincount(tf.reshape(y_train[:,:,:],[-1]), minlength=0)\n",
    "initial_bias = np.log(pos/neg)\n",
    "print(initial_bias)\n",
    "print('class 0: {} class 1: {}'.format(neg,pos))\n",
    "initial_bias = tf.keras.initializers.Constant(initial_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simple_masking_LSTM(inputA, initial_bias_, **kwargs):\n",
    "\n",
    "#     model = keras.models.Sequential([\n",
    "#         keras.layers.Masking(input_shape=[None,inputA], mask_value=0, name=\"masking_layer\"),\n",
    "#         keras.layers.BatchNormalization(),\n",
    "#         keras.layers.LSTM(100, return_sequences=True, name=\"lstm_1\"),\n",
    "#         keras.layers.LeakyReLU(),\n",
    "#         keras.layers.Dropout(0.3),\n",
    "#         keras.layers.LSTM(100, return_sequences=True, name=\"lstm_2\"),\n",
    "#         keras.layers.Dropout(0.3),\n",
    "#         keras.layers.TimeDistributed(keras.layers.Dense(1, activation='sigmoid', bias_initializer=initial_bias_))\n",
    "# ])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masking_LSTM(inputA, initial_bias_, **kwargs):\n",
    "\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Masking(input_shape=[None,inputA], mask_value=0, name=\"masking_layer\"),\n",
    "        keras.layers.BatchNormalization(momentum=0.999),\n",
    "        keras.layers.LSTM(100, return_sequences=True, kernel_initializer=\"he_normal\"),\n",
    "        keras.layers.LeakyReLU(),\n",
    "#         keras.layers.Dropout(0.3),\n",
    "        keras.layers.LSTM(100, return_sequences=True, kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01),name=\"lstm_2\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dense(1, activation='sigmoid', bias_initializer=initial_bias_))\n",
    "])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM_masking_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_layer (Masking)      (None, None, 536)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, None, 536)         2144      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 100)         254800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, None, 100)         80400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 1)           101       \n",
      "=================================================================\n",
      "Total params: 337,445\n",
      "Trainable params: 336,373\n",
      "Non-trainable params: 1,072\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = masking_LSTM(X_train.shape[-1],initial_bias)\n",
    "model._name='LSTM_masking_model'\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.01**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.001, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_lstm_logs/run_2020_05_03-17_19_57\n",
      "Train on 118950 samples, validate on 39651 samples\n",
      "Epoch 1/10\n",
      "118950/118950 [==============================] - 372s 3ms/sample - loss: 0.1780 - accuracy: 0.9908 - binary_crossentropy: 0.0464 - Precision: 0.8793 - Recall: 0.0403 - val_loss: 0.0016 - val_accuracy: 0.9909 - val_binary_crossentropy: 0.0264 - val_Precision: 1.0000 - val_Recall: 0.0243\n",
      "Epoch 2/10\n",
      "118950/118950 [==============================] - 363s 3ms/sample - loss: 6.9288e-04 - accuracy: 0.9979 - binary_crossentropy: 0.0068 - Precision: 0.9980 - Recall: 0.7844 - val_loss: 4.2151e-04 - val_accuracy: 0.9997 - val_binary_crossentropy: 0.0029 - val_Precision: 0.9950 - val_Recall: 0.9733\n",
      "Epoch 3/10\n",
      "118950/118950 [==============================] - 352s 3ms/sample - loss: 3.5398e-04 - accuracy: 0.9998 - binary_crossentropy: 0.0024 - Precision: 0.9984 - Recall: 0.9771 - val_loss: 2.4999e-04 - val_accuracy: 0.9997 - val_binary_crossentropy: 0.0027 - val_Precision: 0.9902 - val_Recall: 0.9782\n",
      "Epoch 4/10\n",
      "118950/118950 [==============================] - 354s 3ms/sample - loss: 2.2701e-04 - accuracy: 0.9999 - binary_crossentropy: 0.0014 - Precision: 0.9984 - Recall: 0.9889 - val_loss: 1.7277e-04 - val_accuracy: 0.9998 - val_binary_crossentropy: 0.0018 - val_Precision: 0.9951 - val_Recall: 0.9830\n",
      "Epoch 5/10\n",
      "118950/118950 [==============================] - 352s 3ms/sample - loss: 1.5947e-04 - accuracy: 1.0000 - binary_crossentropy: 9.5601e-04 - Precision: 1.0000 - Recall: 0.9976 - val_loss: 1.8702e-04 - val_accuracy: 0.9998 - val_binary_crossentropy: 6.5381e-04 - val_Precision: 0.9856 - val_Recall: 0.9951\n",
      "Epoch 6/10\n",
      "118950/118950 [==============================] - 344s 3ms/sample - loss: 1.2452e-04 - accuracy: 1.0000 - binary_crossentropy: 7.2808e-04 - Precision: 1.0000 - Recall: 0.9984 - val_loss: 1.1744e-04 - val_accuracy: 0.9998 - val_binary_crossentropy: 8.4435e-04 - val_Precision: 0.9879 - val_Recall: 0.9927\n",
      "Epoch 7/10\n",
      "118950/118950 [==============================] - 340s 3ms/sample - loss: 1.0308e-04 - accuracy: 1.0000 - binary_crossentropy: 5.8356e-04 - Precision: 1.0000 - Recall: 1.0000 - val_loss: 9.5848e-05 - val_accuracy: 0.9999 - val_binary_crossentropy: 8.2581e-04 - val_Precision: 0.9976 - val_Recall: 0.9927\n",
      "Epoch 8/10\n",
      "118950/118950 [==============================] - 344s 3ms/sample - loss: 8.7917e-05 - accuracy: 1.0000 - binary_crossentropy: 5.0758e-04 - Precision: 1.0000 - Recall: 1.0000 - val_loss: 1.1548e-04 - val_accuracy: 0.9998 - val_binary_crossentropy: 9.1078e-04 - val_Precision: 0.9809 - val_Recall: 0.9951\n",
      "Epoch 9/10\n",
      "118950/118950 [==============================] - 342s 3ms/sample - loss: 7.9577e-05 - accuracy: 1.0000 - binary_crossentropy: 4.5729e-04 - Precision: 1.0000 - Recall: 0.9992 - val_loss: 9.0348e-05 - val_accuracy: 0.9999 - val_binary_crossentropy: 5.2406e-04 - val_Precision: 0.9976 - val_Recall: 0.9951\n",
      "Epoch 10/10\n",
      "118950/118950 [==============================] - 326s 3ms/sample - loss: 7.0829e-05 - accuracy: 1.0000 - binary_crossentropy: 4.0797e-04 - Precision: 1.0000 - Recall: 1.0000 - val_loss: 7.7223e-05 - val_accuracy: 0.9999 - val_binary_crossentropy: 7.7710e-04 - val_Precision: 1.0000 - val_Recall: 0.9903\n"
     ]
    }
   ],
   "source": [
    "prep_run()\n",
    "run_logdir = get_run_logdir(\"test_lstm_logs\")\n",
    "print(run_logdir)\n",
    "\n",
    "tb = TensorBoard(run_logdir)\n",
    "\n",
    "red_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='binary_crossentropy',\n",
    "                                          patience=3, \n",
    "                                          verbose=1,\n",
    "                                          factor=0.005, \n",
    "                                          min_lr=0.00001)\n",
    "lr_sched = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='binary_crossentropy', \n",
    "                                      patience=3, restore_best_weights=True)\n",
    "\n",
    "# adam: clipvalue=0.5, lr=0.001, beta_1=0.9, beta_2=0.999\n",
    "model.compile(loss=\"binary_crossentropy\", \n",
    "                           optimizer=keras.optimizers.Adam(), \n",
    "                           metrics=[\"accuracy\", \"binary_crossentropy\", \n",
    "                                    \"Precision\", \"Recall\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "#                                  steps_per_epoch=resampled_steps_per_epoch, \n",
    "                                 epochs=10, callbacks=[tb, es, red_lr, lr_sched]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"val_loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "#     plt.axis([1, 20, 0, 0.05])\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plot_learning_curves(history.history['loss'],history.history['val_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39651/39651 [==============================] - 33s 842us/sample - loss: 7.7223e-05 - accuracy: 0.9999 - binary_crossentropy: 7.7710e-04 - Precision: 1.0000 - Recall: 0.9903\n"
     ]
    }
   ],
   "source": [
    "# ?test_masking_model.evaluate\n",
    "# print('Evaluating...')\n",
    "scores = model.evaluate(X_val[:,:,:], y_val[:,:,:])\n",
    "# # print(\"Metrics: accuracy, binary_crossentropy, Precision, Recall: \",scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_val[:3,:,:])\n",
    "y_pred[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: (39651, 26, 1) y_actual: (39651, 26, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1030514\n",
      "           1       1.00      0.99      1.00       412\n",
      "\n",
      "    accuracy                           1.00   1030926\n",
      "   macro avg       1.00      1.00      1.00   1030926\n",
      "weighted avg       1.00      1.00      1.00   1030926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict_classes(X_val[:,:,:])\n",
    "y_actual = y_val[:,:,:]\n",
    "print('y_pred: {} y_actual: {}'.format(y_pred.shape, y_actual.shape))\n",
    "print(classification_report(tf.reshape(y_actual,[-1]), tf.reshape(y_pred,[-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the LSTM on ecommerce sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: (67973, 26, 1) y_actual: (67973, 26, 1)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   1766548\n",
      "           1       0.98      0.99      0.99       750\n",
      "\n",
      "    accuracy                           1.00   1767298\n",
      "   macro avg       0.99      1.00      0.99   1767298\n",
      "weighted avg       1.00      1.00      1.00   1767298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_test[:,:,:])\n",
    "y_actual = y_test[:,:,:]\n",
    "\n",
    "print('y_pred: {} y_actual: {}'.format(y_pred.shape, y_actual.shape))\n",
    "print(classification_report(tf.reshape(y_actual,[-1]), tf.reshape(y_pred,[-1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46 entries, 0 to 49\n",
      "Data columns (total 5 columns):\n",
      "visitorid    46 non-null int64\n",
      "event        46 non-null object\n",
      "itemid       46 non-null int64\n",
      "parentid     46 non-null float64\n",
      "date         46 non-null datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(1)\n",
      "memory usage: 2.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data = df[:50].copy()\n",
    "data.drop(columns=['timestamp', 'categoryid', 'sessionid', 'transactionid',\n",
    "                   'x0_weekday_0', 'x0_weekday_1','x0_weekday_2',\n",
    "                   'x0_weekday_3','x0_weekday_4','x0_weekday_5','x0_weekday_6'], inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.info()\n",
    "# a = pd.data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReshapeDataset(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, full_data):\n",
    "#         pass\n",
    "        self.full_data = full_data\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # create an empty df with all category vals\n",
    "        self.cat_df = pd.DataFrame(columns=self.full_data.parentid.unique().tolist())\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        # create 3 dfs for views, adds, purch\n",
    "        \n",
    "        # views\n",
    "        # removing sample visitorid in (552148, 189384) \n",
    "        print('Creating views pivot table...')\n",
    "        self.df_sample = X.query('event == \"view\" ').copy()\n",
    "        self.df_views = pd.pivot_table(self.df_sample, index=['date','visitorid'], \n",
    "                                columns=['parentid'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_views = self.df_views.reset_index()\n",
    "        print('views df shape: ',self.df_views.shape)\n",
    "        print('Merging parentID dataframe to add-to-cart...')\n",
    "        if len(self.df_views) == 0:\n",
    "            self.df_views = pd.DataFrame(0,  index = np.arange(1), columns=['date','visitorid'])\n",
    "            self.df_fullviews = pd.concat([self.df_views,self.cat_df], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            self.df_fullviews = self.df_views.merge(self.cat_df, how='outer', on=None)\n",
    "        \n",
    "        # add to cart\n",
    "        print('Creating add-to-cart pivot table...')\n",
    "        self.df_sample2 = X.query('event == \"addtocart\" ').copy()\n",
    "        self.df_adds = pd.pivot_table(self.df_sample2, index=['date','visitorid'], \n",
    "                                columns=['parentid'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_adds = self.df_adds.reset_index()\n",
    "        print('adds df shape: ',self.df_adds.shape)\n",
    "        \n",
    "        # purchases\n",
    "        print('Creating transactions pivot table...')\n",
    "        self.df_sample3 = X.query('event == \"transaction\" ').copy()\n",
    "        self.df_convert = pd.pivot_table(self.df_sample3, index=[ 'date','visitorid'], \n",
    "                                columns=['event'], values='itemid',aggfunc='count', fill_value=0)\n",
    "        self.df_convert = self.df_convert.reset_index()\n",
    "        \n",
    "        # join views and cart to category df \n",
    "        print('Merging parentID dataframe to add-to-cart...')\n",
    "        \n",
    "        if len(self.df_adds) == 0:\n",
    "            self.df_adds = pd.DataFrame(0,  index = np.arange(1), columns=['date','visitorid'])\n",
    "            self.df_fulladds = pd.concat([self.df_adds,self.cat_df], axis=0, ignore_index=True)\n",
    "        else:\n",
    "            self.df_fulladds = self.df_adds.merge(self.cat_df, how='outer', on=None)\n",
    "        \n",
    "        \n",
    "        # merge both views and cart together\n",
    "        print('...and merging views and add to cart dfs together.')\n",
    "        self.full_df = self.df_fullviews.merge(self.df_fulladds, \n",
    "                                          how='outer', on=['date','visitorid'],\n",
    "                                          suffixes=('_views', '_adds'))\n",
    "        \n",
    "#         if len(self.df_convert) > 0:\n",
    "        print('Creating target vector...')\n",
    "        # create our target vector by changing column values to binary\n",
    "        self.df_convert['transaction'] = np.where(self.df_convert['transaction']>0,1,0)\n",
    "        # merge target DF with the main features DF to make it easier \n",
    "        # to create our final input and target arrays.\n",
    "        self.full_df = self.full_df.merge(self.df_convert, how=\"outer\", on=[\"visitorid\", \"date\"])\n",
    "        self.full_df.fillna(0, inplace=True)\n",
    "        \n",
    "        print('Returning results. Process complete.')\n",
    "        return self.full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index  visitorid event  itemid  parentid       date\n",
      "0      0     257597  view  355908     805.0 2015-06-02\n",
      "Creating views pivot table...\n",
      "views df shape:  (1, 3)\n",
      "Merging parentID dataframe to add-to-cart...\n",
      "Creating add-to-cart pivot table...\n",
      "adds df shape:  (0, 2)\n",
      "Creating transactions pivot table...\n",
      "Merging parentID dataframe to add-to-cart...\n",
      "...and merging views and add to cart dfs together.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns] and float64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-899-47c9d6622589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen_df\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshapeDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#     x = CreateInputArray().fit_transform(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-898-bebfe3ef4e85>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     57\u001b[0m         self.full_df = self.df_fullviews.merge(self.df_fulladds, \n\u001b[1;32m     58\u001b[0m                                           \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'outer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'visitorid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                           suffixes=('_views', '_adds'))\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#         if len(self.df_convert) > 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7347\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7348\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7349\u001b[0;31m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7350\u001b[0m         )\n\u001b[1;32m   7351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     )\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;31m# to avoid incompat dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on datetime64[ns] and float64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "users = data.visitorid.unique()\n",
    "\n",
    "def df_gen(users):\n",
    "    global data\n",
    "    global df\n",
    "    for i in users:\n",
    "        grouped = data[data.visitorid == i]\n",
    "#         grouped = ReshapeDataset().fit_transform(grouped)\n",
    "        yield grouped.reset_index()\n",
    "    \n",
    "gen_df = df_gen(users)\n",
    "\n",
    "for x in gen_df:\n",
    "    print(x)\n",
    "    x = ReshapeDataset(df).fit_transform(x)\n",
    "    x = x.to_numpy()\n",
    "#     x = CreateInputArray().fit_transform(x)\n",
    "#     print('User DF:')\n",
    "#     print(x[:,:3])\n",
    "    print('X shape: ',x.shape)\n",
    "    \n",
    "# gen_X = tf.data.Dataset.from_generator(df_gen,(tf.int64, tf.int64, tf.string, tf.int64, tf.float64, tf.string), args=[users])\n",
    "# for x in gen_X:\n",
    "#     print('User DF:')\n",
    "#     x = ReshapeDataset().fit_transform(x)\n",
    "#     print(x.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>visitorid</th>\n",
       "      <th>event</th>\n",
       "      <th>itemid</th>\n",
       "      <th>transactionid</th>\n",
       "      <th>categoryid</th>\n",
       "      <th>parentid</th>\n",
       "      <th>sessionid</th>\n",
       "      <th>date</th>\n",
       "      <th>x0_weekday_0</th>\n",
       "      <th>x0_weekday_1</th>\n",
       "      <th>x0_weekday_2</th>\n",
       "      <th>x0_weekday_3</th>\n",
       "      <th>x0_weekday_4</th>\n",
       "      <th>x0_weekday_5</th>\n",
       "      <th>x0_weekday_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-06-02 01:02:12</td>\n",
       "      <td>257597</td>\n",
       "      <td>view</td>\n",
       "      <td>355908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>2015-06-02_257597</td>\n",
       "      <td>2015-06-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  visitorid event  itemid  transactionid  categoryid  \\\n",
       "0  2015-06-02 01:02:12     257597  view  355908            NaN      1173.0   \n",
       "\n",
       "   parentid          sessionid       date  x0_weekday_0  x0_weekday_1  \\\n",
       "0     805.0  2015-06-02_257597 2015-06-02           NaN           1.0   \n",
       "\n",
       "   x0_weekday_2  x0_weekday_3  x0_weekday_4  x0_weekday_5  x0_weekday_6  \n",
       "0           NaN           NaN           NaN           NaN           NaN  "
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.visitorid == 257597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Creates a `Dataset` whose elements are generated by `generator`.\n",
       "\n",
       "The `generator` argument must be a callable object that returns\n",
       "an object that supports the `iter()` protocol (e.g. a generator function).\n",
       "The elements generated by `generator` must be compatible with the given\n",
       "`output_types` and (optional) `output_shapes` arguments.\n",
       "\n",
       ">>> import itertools\n",
       ">>>\n",
       ">>> def gen():\n",
       "...   for i in itertools.count(1):\n",
       "...     yield (i, [1] * i)\n",
       ">>>\n",
       ">>> dataset = tf.data.Dataset.from_generator(\n",
       "...      gen,\n",
       "...      (tf.int64, tf.int64),\n",
       "...      (tf.TensorShape([]), tf.TensorShape([None])))\n",
       ">>>\n",
       ">>> list(dataset.take(3).as_numpy_iterator())\n",
       "[(1, array([1])), (2, array([1, 1])), (3, array([1, 1, 1]))]\n",
       "\n",
       "NOTE: The current implementation of `Dataset.from_generator()` uses\n",
       "`tf.numpy_function` and inherits the same constraints. In particular, it\n",
       "requires the `Dataset`- and `Iterator`-related operations to be placed\n",
       "on a device in the same process as the Python program that called\n",
       "`Dataset.from_generator()`. The body of `generator` will not be\n",
       "serialized in a `GraphDef`, and you should not use this method if you\n",
       "need to serialize your model and restore it in a different environment.\n",
       "\n",
       "NOTE: If `generator` depends on mutable global variables or other external\n",
       "state, be aware that the runtime may invoke `generator` multiple times\n",
       "(in order to support repeating the `Dataset`) and at any time\n",
       "between the call to `Dataset.from_generator()` and the production of the\n",
       "first element from the generator. Mutating global variables or external\n",
       "state can cause undefined behavior, and we recommend that you explicitly\n",
       "cache any external state in `generator` before calling\n",
       "`Dataset.from_generator()`.\n",
       "\n",
       "Args:\n",
       "  generator: A callable object that returns an object that supports the\n",
       "    `iter()` protocol. If `args` is not specified, `generator` must take no\n",
       "    arguments; otherwise it must take as many arguments as there are values\n",
       "    in `args`.\n",
       "  output_types: A nested structure of `tf.DType` objects corresponding to\n",
       "    each component of an element yielded by `generator`.\n",
       "  output_shapes: (Optional.) A nested structure of `tf.TensorShape` objects\n",
       "    corresponding to each component of an element yielded by `generator`.\n",
       "  args: (Optional.) A tuple of `tf.Tensor` objects that will be evaluated\n",
       "    and passed to `generator` as NumPy-array arguments.\n",
       "\n",
       "Returns:\n",
       "  Dataset: A `Dataset`.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tf.data.Dataset.from_generator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
